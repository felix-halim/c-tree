<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<style type="text/css">
.axis path,
.axis line {
  fill: none;
  stroke: black;
}
.line {
  fill: none;
  stroke: steelblue;
  stroke-width: 1.5px;
}
</style>
<script src="d3.v3.min.js"></script>
<script src="data.js?3"></script>
<script src="linechart.js?2"></script>
</head>

<body onload="renderLinecharts()">

<h1>Do We Really Need to Pre-Sort?</h1>

<p>All experiments uses N = 10^8 random sparse 32 bit integer.<br>
The queries are random point queries.<br>
The updates are random point updates in the same 32 bit integer domain.<br>
</p>

<h2>Racing for Queries</h2>

<p>Given a certain amount of time, how many queries can an algorithm answers?<br>
The setup is: insert 10^8 random integers and perform 10^9 point queries.
</p>

<div class="chart" style="float:left; display: inline-block">{
  width: 550, height: 250,
  yAxis: { label: 'Query Sequence',  attr: 'Q', scale: 'log', domain: [0.5, 2e9], format: ticksPow10, ticks: [1, 10, 1e2, 1e3, 1e4, 1e5, 1e6, 1e7, 1e8, 1e9], margin: 40 },
  xAxis: { label: 'NOUP: Ratio to Sort', attr: 'total_time', scale: 'log', domain: [0.4, 4000], format: 'd', ticks: [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50, 60, 70, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 2000, 3000, 4000], margin: 40 },
  legend: {
    'sort' : { x: 300, y: 150 },
    'art' : { x: 300, y: 130 },
    'art_best_eager' : { x: 300, y: 110 },
    'btree_google' : { x: 300, y: 90 },
    'crack' : { x: 100, y: 180 },
    'comb' : { x: 40, y: 120 },
  },
  data: group(filter([
    { attr: 'update_workload',   values: ['NOUP'] },
    { attr: 'algorithm',         values: ['sort', 'comb', 'crack', 'art_best_eager', 'art', 'btree_google'] },
  ]), 'algorithm'),
}</div>

<div style="display:none">
<br><br>
<input type="checkbox"> BTree (Google)<br>
<input type="checkbox"> CTree 32<br>
<input type="checkbox"> CTree 1024<br>
<input type="checkbox"> CTree 4096<br>
<input type="checkbox"> CTree Eager (like bulk insert BTree)<br>
<input type="checkbox"> Crack<br>
<input type="checkbox"> COMB<br>
<input type="checkbox"> Sort<br>
</div>

<p style="clear:both">
<!-- Cracking ART (ARTC) answered 10M queries during the first 30 seconds.<br>
ART starts answering the first query after 29.3 seconds.<br>
BTree (BT) starts answering the first query after 54.5 seconds.<br>
In the end Cracking ART and ART finishes the 10^9 queries almost together.<br>
Cracking BTree (BTC) has similar performance with Cracking ART and thus omitted in the graph to reduce clutter.
 --><br>




<h2>Read Only Queries</h2>

<div class="chart" style="display: inline-block">{
  xAxis: { label: 'Query Sequence',  attr: 'Q', scale: 'log', domain: [0.5, 2e9], format: ticksPow10, ticks: [1, 10, 1e2, 1e3, 1e4, 1e5, 1e6, 1e7, 1e8, 1e9], margin: 40 },
  yAxis: { label: 'NOUP: Ratio to Sort', attr: 'total_time', scale: 'log', domain: [0.05, 10], format: 'pow10', ticks: [0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], margin: 40 },
  legend: {
    'sort' : { x: 50, y: 50 },
    'crack' : { x: 250, y: 20 },
    'comb' : { x: 130, y: 120 },
  },
  base: 'sort',
  data: group(filter([
    { attr: 'update_workload',   values: ['NOUP'] },
    { attr: 'algorithm',         values: ['sort', 'comb', 'crack'] },
  ]), 'algorithm'),
}</div>

<div class="chart" style="display: inline-block">{
  xAxis: { label: 'Query Sequence',  attr: 'Q', scale: 'log', domain: [0.5, 2e9], format: ticksPow10, ticks: [1, 10, 1e2, 1e3, 1e4, 1e5, 1e6, 1e7, 1e8, 1e9], margin: 40 },
  yAxis: { label: 'NOUP: Ratio to Sort', attr: 'total_time', scale: 'log', domain: [0.05, 10], format: 'pow10', ticks: [0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], margin: 40 },
  legend: {
    'sort' : { x: 50, y: 50 },
    'crack' : { x: 250, y: 20 },
    'ctree_32_64' : { x: 30, y: 90 },
    'ctree_32_1024' : { x: 120, y: 110 },
  },
  base: 'sort',
  data: group(filter([
    { attr: 'update_workload',   values: ['NOUP'] },
    { attr: 'algorithm',         values: ['sort', 'crack', 'ctree_32_64', 'ctree_32_4096'] },
  ]), 'algorithm'),
}</div>

<div class="chart" style="display: inline-block">{
  xAxis: { label: 'Query Sequence',  attr: 'Q', scale: 'log', domain: [0.5, 2e9], format: ticksPow10, ticks: [1, 10, 1e2, 1e3, 1e4, 1e5, 1e6, 1e7, 1e8, 1e9], margin: 40 },
  yAxis: { label: 'NOUP: Ratio to Sort', attr: 'total_time', scale: 'log', domain: [0.05, 10], format: 'pow10', ticks: [0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], margin: 40 },
  legend: {
    'sort' : { x: 50, y: 50 },
    'art_best' : { x: 70, y: 125 },
    'art_best_eager' : { x: 250, y: 100 },
    'ctree_32_64' : { x: 30, y: 90 },
  },
  base: 'sort',
  data: group(filter([
    { attr: 'update_workload',   values: ['NOUP'] },
    { attr: 'algorithm',         values: ['sort', 'art_best_eager', 'art_best', 'ctree_32_64'] },
  ]), 'algorithm'),
}</div>


<div class="chart" style="display: inline-block">{
  xAxis: { label: 'Query Sequence',  attr: 'Q', scale: 'log', domain: [0.5, 2e9], format: ticksPow10, ticks: [1, 10, 1e2, 1e3, 1e4, 1e5, 1e6, 1e7, 1e8, 1e9], margin: 40 },
  yAxis: { label: 'NOUP: Ratio to Sort', attr: 'total_time', scale: 'log', domain: [0.7, 5], format: 'pow10', ticks: [0.7, 0.8, 0.9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], margin: 40 },
  legend: {
    'crack' : { x: 265, y: 60 },
    'ctree_32_64' : { x: 50, y: 40 },
    'ctree_32_1024' : { x: 120, y: 110 },
    'ctree_32_4096' : { x: 250, y: 105 },
    'comb' : { x: 50, y: 140 },
  },
  base: 'comb',
  data: group(filter([
    { attr: 'update_workload',   values: ['NOUP'] },
    { attr: 'algorithm',         values: ['comb', 'crack', 'ctree_32_64', 'ctree_32_1024', 'ctree_32_4096'] },
  ]), 'algorithm'),
}</div>

<p>Cumulative runtime compared to Sort under read only queries.
<ul>
<li>COMB is consistently faster than Crack on 10th+ queries.
<li>COMB is <b>never worse</b> than Sort on any number of queries.
<li>ART (with bulk insert) is faster than std::sort in initialization costs.
<li>ART is perforamnce is consistently fast O(K) per query.
<li>ART wins for large number of queries.
<li>Note for ART: if K is large, it may be a different story.
<li>Cracked ART (ARTC): has low initialization cost while preserve fast cumulative cost.
</ul>
</p>



<h2>Low Frequency High Volume (LFHV) Updates</h2>

<div class="chart" style="display: inline-block">{
  xAxis: { label: 'Query Sequence',  attr: 'Q', scale: 'log', domain: [0.5, 2e9], format: ticksPow10, ticks: [1, 10, 1e2, 1e3, 1e4, 1e5, 1e6, 1e7, 1e8, 1e9 ], margin: 40 },
  yAxis: { label: 'LFHV: Ratio to CTree', attr: 'total_time', scale: 'log', domain: [0.3, 10], format: 'pow10', ticks: [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], margin: 30 },
  base: 'ctree_32_64',
  legend: {
    'ctree_32_1024':  { x: 120, y: 20 },
    'crack':          { x: 120, y: 40 },
    'comb':           { x: 120, y: 60 },
    'ctree_32_64':    { x: 120, y: 80 },
  },
  data: group(filter([
    { attr: 'update_workload',   values: ['LFHV'] },
    { attr: 'algorithm',         values: ['crack', 'comb', 'ctree_32_64', 'ctree_32_1024'] },
  ]), 'algorithm'),
}</div>

<div class="chart" style="display: inline-block">{
  xAxis: { label: 'Query Sequence',  attr: 'Q', scale: 'log', domain: [0.5, 2e9], format: ticksPow10, ticks: [1, 10, 1e2, 1e3, 1e4, 1e5, 1e6, 1e7, 1e8, 1e9 ], margin: 40 },
  yAxis: { label: 'LFHV: Ratio to CTree', attr: 'total_time', scale: 'log', domain: [0.3, 10], format: 'pow10', ticks: [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 2, 3, 4, 5, 10], margin: 30 },
  base: 'ctree_32_64',
  legend: {
    'comb':           { x: 240, y: 40 },
    'art_best':    { x: 40, y: 80 },
    'art_best_eager':  { x: 55, y: 20 },
    'ctree_32_64':  { x: 260, y: 85 },
  },
  data: group(filter([
    { attr: 'update_workload',   values: ['LFHV'] },
    { attr: 'algorithm',         values: ['comb', 'art_best', 'art_best_eager', 'ctree_32_64'] },
  ]), 'algorithm'),
}</div>


<p>Cumulative runtime compared to CTree under LFHV updates.
<ul>
<li>COMB is consistently faster than Crack on 10th+ queries.
<li>COMB does not eliminate the shuffling overhead, it reduces it down to O(sqrt(indexes)).
<li>CTree does not have shuffling overhead, it wins in the long run on heavy updates.
<li>CTree with 64 bucket size (CT64) is worse than COMB due inefficient Fusion.
<li>CTree with 1024 bucket size (CT1024) has better Fusion thus has similar performance to COMB for the first 10^5 queries<br>
but worse in the long run because of the high cost in updating large Bucket without (mini cracker) indexes.
<li>CTree can be modified to be "adaptive" in terms of bucket size (need more research).
</ul>
</p>

<h2>Streaming Insert Interleaved with Random Count Queries</h2>
<div class="chart">{
width: 255,
xAxis: { label: 'Query Sequence', attr: 'Q', scale: 'log', domain: [0.5, 2e3], format: null, ticks: [1, 10, 1e2, 1e3], margin: 40 },
yAxis: { label: 'Cumulative Runtime', attr: 'total_time', scale: 'logx', domain: [-50, 500], format: 'pow10', ticks: [0, 100, 200, 300, 400, 500], margin: 40 },
legend: {
'crack_count' : { x: 50, y: 30 },
'comb_count' : { x: 50, y: 50 },
},
data: group(filter([
{ attr: 'update_workload', values: ['APPEND'] },
{ attr: 'query_workload', values: ['Random'] },
{ attr: 'selectivity', values: ['0.001000'] },
{ attr: 'algorithm', values: ['crack_count', 'comb_count'] },
]), 'algorithm'),
}</div>

<p>Insert 100K random tuples before each random COUNT(*) query with 0.1% selectivity.<br>
After 1000 inserts, the total number of tuples is 100M.<br>
Each count query requires the algo to iterate for each pieces to acquire it's size (including COMB).
<ul>
<li>Crack needs to shuffle in the pending insert for each query in range [a, b).
<li>If there is not enough room to shuffle in, then some elements is swapped with the pending insert (see MRI algorithm).
<li>This causes "trashing" effect where tuples constantly go back and forth to and from the array and the pending insert structure (AVL tree).
<li>COMB lazy insert appends or chain inserts to the correct bucket, no trashing.
<li>The result is that Crack gets worse rapidly as the size of the pending insert grows, and
<li>COMB stays resilient. It completes all 1000 inserts interleaved with a random count query in ~8 seconds while Crack in 445 seconds.
</ul>
</p>


<h2>Fusion Effectiveness on Different Bucket Sizes</h2>
<p>TODO
<ul>
</ul>
</p>

<!-- <div class="chart">{
  xAxis: { label: 'Query Sequence',  attr: 'Q', scale: 'log', domain: [0.5, 2e9], format: ticksPow10, ticks: [1, 10, 1e2, 1e3, 1e4, 1e5, 1e6, 1e7, 1e8, 1e9], margin: 40 },
  yAxis: { label: 'Ratio to Sort', attr: 'total_time', scale: 'logx', domain: [0, 4], format: noFormat, ticks: [0, 1, 2, 3, 4], margin: 40 },
  base: 'sort',
  legend: {
    'sort' : { x: 100, y: 10 },
    'crack' : { x: 100, y: 30 },
    'comb' : { x: 100, y: 50 },
    'ctree_32_4096' : { x: 100, y: 70 },
  },
  data: group(filter([
    { attr: 'update_workload',   values: ['NOUP'] },
    { attr: 'algorithm',         values: [
      'sort',
      'crack',
      'comb',
      'ctree_32_1024',
      'ctree_32_4096',
    ] },
  ]), 'algorithm'),
}</div>


<div class="chart">{
  xAxis: { label: 'Query Sequence',  attr: 'Q', scale: 'log', domain: [0.5, 1e9], format: ticksPow10, ticks: [1, 10, 1e2, 1e3, 1e4, 1e5, 1e6, 1e7, 1e8, 1e9], margin: 40 },
  yAxis: { label: 'Cumulative Runtime', attr: 'total_time', scale: 'logx', domain: [0.1, 7], format: d3.format('s'), ticks: [0, 1, 2, 3, 4, 5, 6, 7], margin: 40 },
  base: 'ctree_32_64',
  legend: {
    'ctree_32_64' : { x: 100, y: 10 },
    'crack' : { x: 100, y: 30 },
    'comb' : { x: 100, y: 50 },
  },
  data: group(filter([
    { attr: 'update_workload',   values: ['QUEUE'] },
    { attr: 'algorithm',         values: [
      'crack',
      'comb',
      'ctree_32_64',
    ] },
  ]), 'algorithm'),
}</div>


<div class="chart">{
  xAxis: { label: 'Query Sequence',  attr: 'Q', scale: 'log', domain: [0.5, 1e9], format: ticksPow10, ticks: [1, 10, 1e2, 1e3, 1e4, 1e5, 1e6, 1e7, 1e8, 1e9], margin: 40 },
  yAxis: { label: 'Cumulative Runtime', attr: 'total_time', scale: 'logx', domain: [0.1, 7], format: d3.format('s'), ticks: [0, 1, 2, 3, 4, 5, 6, 7], margin: 40 },
  base: 'ctree_32_64',
  legend: {
    'ctree_32_64' : { x: 100, y: 10 },
    'crack' : { x: 100, y: 30 },
    'comb' : { x: 100, y: 50 },
    'ctree_32_4096' : { x: 100, y: 70 },
    'ctree_32_1024' : { x: 100, y: 70 },
  },
  data: group(filter([
    { attr: 'update_workload',   values: ['TRASH'] },
    { attr: 'algorithm',         values: [
      'crack',
      'comb',
      'ctree_32_64',
      'ctree_32_1024',
      'ctree_32_4096',
    ] },
  ]), 'algorithm'),
}</div>
 --></body>
