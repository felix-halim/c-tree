<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<style type="text/css">
.axis path,
.axis line {
  fill: none;
  stroke: black;
}
.line {
  fill: none;
  stroke: steelblue;
  stroke-width: 1.5px;
}
</style>
<script src="d3.v3.min.js"></script>
<script src="data.js?1"></script>
<script src="linechart.js?1"></script>
<script>
data = d3.csv.parse(data);

data.forEach(function (d) {
  d.Q = parseInt(d.Q);
  d.insert_time = parseFloat(d.insert_time);
  d.query_time = parseFloat(d.query_time) + 1e-9;
  d.update_time = parseFloat(d.update_time) + 1e-9;
  d.total_time = d.insert_time + d.query_time + d.update_time;
});

var algo_name = {
  comb:            { name: "COMB", symbol: "diamond", color: "orange" },
  comb_count:      { name: "COMB", symbol: "diamond", color:"orange" },
  art:             { name: "ART", symbol: "diamond", color: "red", },
  art_best:        { name: "ARTC", symbol: "cross", color: "red", },
  art_best_eager:  { name: "ART", symbol: "triangle-up", color: "green", },
  sort:            { name: "Sort", symbol: "triangle-down", color: "blue", },
  crack:           { name: "Crack", symbol: "circle", color: "red", },
  crack_count:     { name: "Crack", symbol: "circle", color: "red", },
  mdd1r:           { name: "Scrack", symbol: "circle", color: "red", },
  ctree_32_64:     { name: "CT64", symbol: "square", color: "brown", },
  ctree_32_1024:   { name: "CT1024", symbol: "triangle-up", color: "magenta", },
  ctree_32_4096:   { name: "BT4096", symbol: "diamond", color: "blue", },
  ctree_eager:     { name: "BTE", symbol: "triangle-up", color: "magenta", },
};

function filter(filters) {
  var ret = data;
  filters.forEach(function (filter) {
    ret = ret.filter(function (d) {
      return filter.values.indexOf(d[filter.attr]) != -1;
    });
  });
  return ret;
}

function group(arr, by) {
  var keys = {}, groups = [];
  arr.forEach(function (d) {
    if (!keys[d[by]]) keys[d[by]] = [];
    keys[d[by]].push(d);
  });
  return keys;
  // for (var i in keys) if (keys.hasOwnProperty(i)) {
  //   var a = keys[i];
  //   keys[i] = [];
  //   groups.push({ group: i, label: a.label, x: a.x, y: a.y });
  // }
}

function expTime(t) {
  function f(x) {
    var v = d3.format(".2f")(x) + "";
    while (v.length > 0 && v[v.length - 1] == '0') v = v.substring(0, v.length - 1);
    if (v.length > 0 && v[v.length - 1] == '.') v = v.substring(0, v.length - 1);
    console.log(v);
    return v;
  }
  if (t >= 3600) return f(t / 3600) + 'h';
  if (t >= 60) return f(t / 60) + 'm';
  if (t >= 1) return f(t) + 's';
  if (t >= 1e-3) return f(t*1e3) + 'ms';
  if (t >= 1e-6) return f(t*1e6) + 'Âµs';
  return "?";
};

function renderCharts() {
  var charts = document.getElementsByClassName('chart');
  Array.prototype.forEach.call(charts, linechart);
}
</script>
</head>

<body onload="renderCharts()">
<h1>Additional Experiments</h1>

<p>All experiments uses N = 10^8 random sparse 32 bit integer.<br>
The queries are random point queries.<br>
The updates are random point updates in the same 32 bit integer domain.<br>
</p>

<h2>Read Only Queries</h2>

<div class="chart" style="display: inline-block">{
  xAxis: { label: 'Query Sequence',  attr: 'Q', scale: 'log', domain: [0.5, 2e9], format: ticksPow10, ticks: [1, 10, 1e2, 1e3, 1e4, 1e5, 1e6, 1e7, 1e8, 1e9], margin: 40 },
  yAxis: { label: 'NOUP: Ratio to Sort', attr: 'total_time', scale: 'log', domain: [0.05, 10], format: 'pow10', ticks: [0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], margin: 40 },
  legend: {
    'sort' : { x: 50, y: 50 },
    'crack' : { x: 250, y: 20 },
    'comb' : { x: 120, y: 110 },
  },
  base: 'sort',
  data: group(filter([
    { attr: 'update_workload',   values: ['NOUP'] },
    { attr: 'algorithm',         values: ['sort', 'comb', 'crack'] },
  ]), 'algorithm'),
}</div>

<div class="chart" style="display: inline-block">{
  xAxis: { label: 'Query Sequence',  attr: 'Q', scale: 'log', domain: [0.5, 2e9], format: ticksPow10, ticks: [1, 10, 1e2, 1e3, 1e4, 1e5, 1e6, 1e7, 1e8, 1e9], margin: 40 },
  yAxis: { label: 'NOUP: Ratio to Sort', attr: 'total_time', scale: 'log', domain: [0.05, 10], format: 'pow10', ticks: [0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], margin: 40 },
  legend: {
    'sort' : { x: 50, y: 50 },
    'crack' : { x: 250, y: 20 },
    'ctree_32_64' : { x: 30, y: 90 },
    'ctree_32_1024' : { x: 120, y: 110 },
  },
  base: 'sort',
  data: group(filter([
    { attr: 'update_workload',   values: ['NOUP'] },
    { attr: 'algorithm',         values: ['sort', 'crack', 'ctree_32_64', 'ctree_32_1024'] },
  ]), 'algorithm'),
}</div>

<div class="chart" style="display: inline-block">{
  xAxis: { label: 'Query Sequence',  attr: 'Q', scale: 'log', domain: [0.5, 2e9], format: ticksPow10, ticks: [1, 10, 1e2, 1e3, 1e4, 1e5, 1e6, 1e7, 1e8, 1e9], margin: 40 },
  yAxis: { label: 'NOUP: Ratio to Sort', attr: 'total_time', scale: 'log', domain: [0.05, 10], format: 'pow10', ticks: [0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], margin: 40 },
  legend: {
    'sort' : { x: 50, y: 50 },
    'art_best' : { x: 70, y: 125 },
    'art_best_eager' : { x: 250, y: 100 },
    'ctree_32_64' : { x: 30, y: 90 },
  },
  base: 'sort',
  data: group(filter([
    { attr: 'update_workload',   values: ['NOUP'] },
    { attr: 'algorithm',         values: ['sort', 'art_best_eager', 'art_best', 'ctree_32_64'] },
  ]), 'algorithm'),
}</div>

<p>Cumulative runtime compared to Sort under read only queries.
<ul>
<li>COMB is consistently faster than Crack on 10th+ queries.
<li>COMB is <b>never worse</b> than Sort on any number of queries.
<li>ART (with bulk insert) is faster than std::sort in initialization costs.
<li>ART is perforamnce is consistently fast O(K) per query.
<li>ART wins for large number of queries.
<li>Note for ART: if K is large, it may be a different story.
<li>Cracked ART (ARTC): has low initialization cost while preserve fast cumulative cost.
</ul>
</p>



<h2>Low Frequency High Volume (LFHV) Updates</h2>

<div class="chart" style="display: inline-block">{
  xAxis: { label: 'Query Sequence',  attr: 'Q', scale: 'log', domain: [0.5, 2e9], format: ticksPow10, ticks: [1, 10, 1e2, 1e3, 1e4, 1e5, 1e6, 1e7, 1e8, 1e9 ], margin: 40 },
  yAxis: { label: 'LFHV: Ratio to CTree', attr: 'total_time', scale: 'log', domain: [0.3, 10], format: 'pow10', ticks: [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], margin: 30 },
  base: 'ctree_32_64',
  legend: {
    'ctree_32_1024':  { x: 120, y: 20 },
    'crack':          { x: 120, y: 40 },
    'comb':           { x: 120, y: 60 },
    'ctree_32_64':    { x: 120, y: 80 },
  },
  data: group(filter([
    { attr: 'update_workload',   values: ['LFHV'] },
    { attr: 'algorithm',         values: ['crack', 'comb', 'ctree_32_64', 'ctree_32_1024'] },
  ]), 'algorithm'),
}</div>

<div class="chart" style="display: inline-block">{
  xAxis: { label: 'Query Sequence',  attr: 'Q', scale: 'log', domain: [0.5, 2e9], format: ticksPow10, ticks: [1, 10, 1e2, 1e3, 1e4, 1e5, 1e6, 1e7, 1e8, 1e9 ], margin: 40 },
  yAxis: { label: 'LFHV: Ratio to CTree', attr: 'total_time', scale: 'log', domain: [0.3, 10], format: 'pow10', ticks: [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 2, 3, 4, 5, 10], margin: 30 },
  base: 'ctree_32_64',
  legend: {
    'comb':           { x: 240, y: 40 },
    'art_best':    { x: 40, y: 80 },
    'art_best_eager':  { x: 55, y: 20 },
    'ctree_32_64':  { x: 260, y: 85 },
  },
  data: group(filter([
    { attr: 'update_workload',   values: ['LFHV'] },
    { attr: 'algorithm',         values: ['comb', 'art_best', 'art_best_eager', 'ctree_32_64'] },
  ]), 'algorithm'),
}</div>


<p>Cumulative runtime compared to CTree under LFHV updates.
<ul>
<li>COMB is consistently faster than Crack on 10th+ queries.
<li>COMB does not eliminate the shuffling overhead, it reduces it down to O(sqrt(indexes)).
<li>CTree does not have shuffling overhead, it wins in the long run on heavy updates.
<li>CTree with 64 bucket size (CT64) is worse than COMB due inefficient Fusion.
<li>CTree with 1024 bucket size (CT1024) has better Fusion thus has similar performance to COMB for the first 10^5 queries<br>
but worse in the long run because of the high cost in updating large Bucket without (mini cracker) indexes.
<li>CTree can be modified to be "adaptive" in terms of bucket size (need more research).
</ul>
</p>

<h2>Streaming Insert Interleaved with Random Count Queries</h2>
<div class="chart">{
width: 255,
xAxis: { label: 'Query Sequence', attr: 'Q', scale: 'log', domain: [0.5, 2e3], format: null, ticks: [1, 10, 1e2, 1e3], margin: 40 },
yAxis: { label: 'Cumulative Runtime', attr: 'total_time', scale: 'logx', domain: [-50, 500], format: 'pow10', ticks: [0, 100, 200, 300, 400, 500], margin: 40 },
legend: {
'crack_count' : { x: 50, y: 30 },
'comb_count' : { x: 50, y: 50 },
},
data: group(filter([
{ attr: 'update_workload', values: ['APPEND'] },
{ attr: 'query_workload', values: ['Random'] },
{ attr: 'selectivity', values: ['0.001000'] },
{ attr: 'algorithm', values: ['crack_count', 'comb_count'] },
]), 'algorithm'),
}</div>

<p>Insert 100K random tuples before each random COUNT(*) query with 0.1% selectivity.<br>
After 1000 inserts, the total number of tuples is 100M.<br>
Each count query requires the algo to iterate for each pieces to acquire it's size (including COMB).
<ul>
<li>Crack needs to shuffle in the pending insert for each query in range [a, b).
<li>If there is not enough room to shuffle in, then some elements is swapped with the pending insert (see MRI algorithm).
<li>This causes "trashing" effect where tuples constantly go back and forth to and from the array and the pending insert structure (AVL tree).
<li>COMB lazy insert appends or chain inserts to the correct bucket, no trashing.
<li>The result is that Crack gets worse rapidly as the size of the pending insert grows, and
<li>COMB stays resilient. It completes all 1000 inserts interleaved with a random count query in ~8 seconds while Crack in 445 seconds.
</ul>
</p>


<h2>Fusion Effectiveness on Different Bucket Sizes</h2>
<p>TODO
<ul>
</ul>
</p>

<!-- <div class="chart">{
  xAxis: { label: 'Query Sequence',  attr: 'Q', scale: 'log', domain: [0.5, 2e9], format: ticksPow10, ticks: [1, 10, 1e2, 1e3, 1e4, 1e5, 1e6, 1e7, 1e8, 1e9], margin: 40 },
  yAxis: { label: 'Ratio to Sort', attr: 'total_time', scale: 'logx', domain: [0, 4], format: noFormat, ticks: [0, 1, 2, 3, 4], margin: 40 },
  base: 'sort',
  legend: {
    'sort' : { x: 100, y: 10 },
    'crack' : { x: 100, y: 30 },
    'comb' : { x: 100, y: 50 },
    'ctree_32_4096' : { x: 100, y: 70 },
  },
  data: group(filter([
    { attr: 'update_workload',   values: ['NOUP'] },
    { attr: 'algorithm',         values: [
      'sort',
      'crack',
      'comb',
      'ctree_32_1024',
      'ctree_32_4096',
    ] },
  ]), 'algorithm'),
}</div>


<div class="chart">{
  xAxis: { label: 'Query Sequence',  attr: 'Q', scale: 'log', domain: [0.5, 1e9], format: ticksPow10, ticks: [1, 10, 1e2, 1e3, 1e4, 1e5, 1e6, 1e7, 1e8, 1e9], margin: 40 },
  yAxis: { label: 'Cumulative Runtime', attr: 'total_time', scale: 'logx', domain: [0.1, 7], format: d3.format('s'), ticks: [0, 1, 2, 3, 4, 5, 6, 7], margin: 40 },
  base: 'ctree_32_64',
  legend: {
    'ctree_32_64' : { x: 100, y: 10 },
    'crack' : { x: 100, y: 30 },
    'comb' : { x: 100, y: 50 },
  },
  data: group(filter([
    { attr: 'update_workload',   values: ['QUEUE'] },
    { attr: 'algorithm',         values: [
      'crack',
      'comb',
      'ctree_32_64',
    ] },
  ]), 'algorithm'),
}</div>


<div class="chart">{
  xAxis: { label: 'Query Sequence',  attr: 'Q', scale: 'log', domain: [0.5, 1e9], format: ticksPow10, ticks: [1, 10, 1e2, 1e3, 1e4, 1e5, 1e6, 1e7, 1e8, 1e9], margin: 40 },
  yAxis: { label: 'Cumulative Runtime', attr: 'total_time', scale: 'logx', domain: [0.1, 7], format: d3.format('s'), ticks: [0, 1, 2, 3, 4, 5, 6, 7], margin: 40 },
  base: 'ctree_32_64',
  legend: {
    'ctree_32_64' : { x: 100, y: 10 },
    'crack' : { x: 100, y: 30 },
    'comb' : { x: 100, y: 50 },
    'ctree_32_4096' : { x: 100, y: 70 },
    'ctree_32_1024' : { x: 100, y: 70 },
  },
  data: group(filter([
    { attr: 'update_workload',   values: ['TRASH'] },
    { attr: 'algorithm',         values: [
      'crack',
      'comb',
      'ctree_32_64',
      'ctree_32_1024',
      'ctree_32_4096',
    ] },
  ]), 'algorithm'),
}</div>
 --></body>
